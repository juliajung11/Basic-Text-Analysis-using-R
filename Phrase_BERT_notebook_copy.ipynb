{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phrase-BERT-notebook-copy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliajung11/Basic-Text-Analysis-using-R/blob/master/Phrase_BERT_notebook_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Moving from words to phrases when doing NLP\n",
        "- [Abe Handler](https://www.abehandler.com/) University of Colorado, Boulder\n",
        "- [Shufan Wang](https://people.cs.umass.edu/~shufanwang/) University of  Massachusetts, Amherst\n",
        "\n",
        "## Phrase-BERT\n",
        "\n",
        "Learning representations of phrases is important\n",
        "for many tasks, such as semantic parsing, translation, question answering, or general corpus exploration.\n",
        "\n",
        "### 1. Why is BERT insufficient?\n",
        "While pretrained language models such as BERT have led to performance improvements in a variety of NLP tasks, we find that\n",
        "they still struggle to produce semantically meaningful embeddings for shorter linguistic units (sentences and phrases). In fact, BERT, when used as an off-the-shelf model to produce sentence or phrase embeddings, often underperforms simple baselines such as averaging GloVe vectors\n",
        "in semantic textual similarity tasks! That makes BERT less effective for use cases that involve phrases understanding.\n",
        "\n",
        "\n",
        "### 2. Modify BERT to understand phrases\n",
        "We develop Phrase-BERT, by\n",
        "fine-tuning BERT with a contrastive learning objective to produce more powerful phrase embeddings. Specifically, we target two major weaknesses\n",
        "of BERT for phrase embeddings: \n",
        "\n",
        "(1)\n",
        "BERT never sees short texts (e.g., phrases) during pretraining, as its inputs are chunks of 512 tokens;\n",
        "\n",
        "(2) BERT relies heavily on lexical similarity (word content overlap) to determine semantic relatedness. \n",
        "\n",
        "Hence, we construct two datasets of lexically-diverse phrasal paraphrases, and phrases associated with their contexts.\n",
        "We then use the paraphrase data and contextual information to finetune BERT with an contrastive learning objective. The goal is that the embedding model learns to place phrase embeddings close to both their paraphrases\n",
        "and the contexts in which they appear.\n",
        "\n",
        "\n",
        "\n",
        "### 3. Phrase-BERT-based Neural Topic Model (PNTM)\n",
        "Most existing topic models use lists of *unigrams* to describe topics but we believe having phrases into the mix can help corpus exploratin too. Here, we show that phrase-BERT can be easily integrated with an autoencoder model to\n",
        "build a phrase-based neural topic model (PNTM). PNTM is aware of phrase semantics and phrasal diversities and can hence present topics as mixtures of words, phrases and even short sentences. \n",
        "\n",
        "Despite its simple architecture, PNTM outperforms\n",
        "other topic model baselines in our human evaluation studies in terms of topic coherence and topic-to-document relatedness.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d6wxORogR755"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quickstart on Phrase-BERT\n",
        "\n",
        "Phrase-BERT is essentially an encoder to produce meaningful embeddings for phrases. Particularly, given a phrase, Phrase-BERT outputs a 768-dimensional vector. The embedding vector lies in a phrase embedding space that is \"semantically coherent\". Here, \"semantic coherence\" refers to the property that embeddings of semantically similar phrases are placed close while others are placed apart.\n",
        "\n"
      ],
      "metadata": {
        "id": "bjwKfeJqpHSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download the dependencies and then Phrase-BERT:"
      ],
      "metadata": {
        "id": "aDwSCWDG-Q2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==3.0.2\n",
        "!pip install sentence-transformers==0.3.3\n",
        "!git clone https://github.com/sf-wa-326/phrase-bert-topic-model.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkXB1_A6pCjE",
        "outputId": "e8954495-719b-4e5a-a9e6-5c066ce6e3a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.8.1rc1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.96)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.21.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.0.47)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2022.1.18)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: sentence-transformers==0.3.3 in /usr/local/lib/python3.7/dist-packages (0.3.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.3) (3.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.3) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.3) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.3) (4.62.3)\n",
            "Requirement already satisfied: transformers>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.3) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.3) (1.21.5)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers==0.3.3) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2.0->sentence-transformers==0.3.3) (3.10.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.3) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.3) (3.4.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.3) (2022.1.18)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.3) (0.0.47)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.3) (0.8.1rc1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.3) (21.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->sentence-transformers==0.3.3) (0.1.96)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers==0.3.3) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers==0.3.3) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=3.0.2->sentence-transformers==0.3.3) (3.0.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.3) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=3.0.2->sentence-transformers==0.3.3) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.2->sentence-transformers==0.3.3) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers==0.3.3) (3.1.0)\n",
            "fatal: destination path 'phrase-bert-topic-model' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's download a pretrained Phrase-BERT model"
      ],
      "metadata": {
        "id": "YO_x9pGy-Z6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/phrase-bert/phrase-bert/phrase-bert-model.zip\n",
        "!unzip phrase-bert-model.zip -d phrase-bert-model/\n",
        "!rm phrase-bert-model.zip"
      ],
      "metadata": {
        "id": "90Vj3GMJ8tat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff9bde5e-17ee-40b9-ce72-ce64a3db9fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-16 05:22:03--  https://storage.googleapis.com/phrase-bert/phrase-bert/phrase-bert-model.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.152.128, 209.85.145.128, 142.250.125.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.152.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 405370341 (387M) [application/zip]\n",
            "Saving to: ‘phrase-bert-model.zip’\n",
            "\n",
            "phrase-bert-model.z 100%[===================>] 386.59M   134MB/s    in 2.9s    \n",
            "\n",
            "2022-02-16 05:22:06 (134 MB/s) - ‘phrase-bert-model.zip’ saved [405370341/405370341]\n",
            "\n",
            "Archive:  phrase-bert-model.zip\n",
            "replace phrase-bert-model/pooled_context_para_triples_p=0.8/config.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how we can load a model of Phrase-BERT, using the sentence-transformer library:"
      ],
      "metadata": {
        "id": "tJ-GdXdrp1zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model_path = '/content/phrase-bert-model/pooled_context_para_triples_p=0.8'\n",
        "model = SentenceTransformer(model_path)"
      ],
      "metadata": {
        "id": "RX4OYMUhsCON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let us try some phrases and get their embeddings from Phrase-BERT"
      ],
      "metadata": {
        "id": "HRMJ6_q07uRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_list = [ 'play an active role', 'participate actively', 'active lifestyle']\n",
        "phrase_embs = model.encode( phrase_list )\n",
        "[p1, p2, p3] = phrase_embs"
      ],
      "metadata": {
        "id": "vXPMzdLysRlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for phrase, embedding in zip(phrase_list, phrase_embs):\n",
        "    print(\"Phrase:\", phrase)\n",
        "    print(\"Embedding:\", embedding.shape)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQGIIKeZsiU5",
        "outputId": "ba0b5311-fcd3-4522-ba6f-f1aafd4180a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phrase: play an active role\n",
            "Embedding: (768,)\n",
            "\n",
            "Phrase: participate actively\n",
            "Embedding: (768,)\n",
            "\n",
            "Phrase: active lifestyle\n",
            "Embedding: (768,)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we convert phrases into vectors, we can computer their similarity using dot product."
      ],
      "metadata": {
        "id": "T3OBRXYmqL0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(f'The dot product between phrase 1 and 2 is: {np.dot(p1, p2)}') \n",
        "print(f'The dot product between phrase 1 and 3 is: {np.dot(p1, p3)}') \n",
        "print(f'The dot product between phrase 2 and 3 is: {np.dot(p2, p3)}') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbrWaDf3uii9",
        "outputId": "2043f412-4325-4671-8104-30a6564cf786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dot product between phrase 1 and 2 is: 218.43597412109375\n",
            "The dot product between phrase 1 and 3 is: 165.48489379882812\n",
            "The dot product between phrase 2 and 3 is: 160.51705932617188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or we can also use cosine similarity:"
      ],
      "metadata": {
        "id": "KODrnnZsqdOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "from torch import nn\n",
        "cos_sim = nn.CosineSimilarity(dim=0)\n",
        "print(f'The cosine similarity between phrase 1 and 2 is: {cos_sim( torch.tensor(p1), torch.tensor(p2))}')\n",
        "print(f'The cosine similarity between phrase 1 and 3 is: {cos_sim( torch.tensor(p1), torch.tensor(p3))}')\n",
        "print(f'The cosine similarity between phrase 2 and 3 is: {cos_sim( torch.tensor(p2), torch.tensor(p3))}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD53Z1xKunQ8",
        "outputId": "8c92fe02-8af4-4688-b0e5-9bc24d322434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cosine similarity between phrase 1 and 2 is: 0.814253568649292\n",
            "The cosine similarity between phrase 1 and 3 is: 0.6130305528640747\n",
            "The cosine similarity between phrase 2 and 3 is: 0.5848934054374695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Model Case Study: Let's get the data first!"
      ],
      "metadata": {
        "id": "rSaL04bU74bQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contiue using the dataset from convokit:"
      ],
      "metadata": {
        "id": "B0uFw_o3qmR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install convokit==2.5.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qsSjmdx7-FA",
        "outputId": "5739bddc-d6db-446f-9ca2-083b99a31c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: convokit==2.5.2 in /usr/local/lib/python3.7/dist-packages (2.5.2)\n",
            "Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.7/dist-packages (from convokit==2.5.2) (3.2.2)\n",
            "Requirement already satisfied: dill>=0.2.9 in /usr/local/lib/python3.7/dist-packages (from convokit==2.5.2) (0.3.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from convokit==2.5.2) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.7/dist-packages (from convokit==2.5.2) (1.3.5)\n",
            "Requirement already satisfied: clean-text>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from convokit==2.5.2) (0.6.0)\n",
            "Requirement already satisfied: unidecode>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from convokit==2.5.2) (1.3.2)\n",
            "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.7/dist-packages (from convokit==2.5.2) (3.7)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from convokit==2.5.2) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from convokit==2.5.2) (1.1.0)\n",
            "Requirement already satisfied: msgpack-numpy>=0.4.3.2 in /usr/local/lib/python3.7/dist-packages (from convokit==2.5.2) (0.4.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from convokit==2.5.2) (1.0.2)\n",
            "Requirement already satisfied: ftfy<7.0,>=6.0 in /usr/local/lib/python3.7/dist-packages (from clean-text>=0.1.1->convokit==2.5.2) (6.1.1)\n",
            "Requirement already satisfied: emoji<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from clean-text>=0.1.1->convokit==2.5.2) (1.6.3)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.1.1->convokit==2.5.2) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->convokit==2.5.2) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->convokit==2.5.2) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->convokit==2.5.2) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->convokit==2.5.2) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->convokit==2.5.2) (1.21.5)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from msgpack-numpy>=0.4.3.2->convokit==2.5.2) (1.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4->convokit==2.5.2) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4->convokit==2.5.2) (2022.1.18)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4->convokit==2.5.2) (4.62.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->convokit==2.5.2) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->convokit==2.5.2) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->convokit==2.5.2) (3.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (2.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (1.0.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (1.0.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (3.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (0.6.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (1.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (21.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (3.10.0.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (2.4.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (3.3.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (0.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (0.9.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (8.0.13)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (3.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (2.23.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (0.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit==2.5.2) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=2.3.5->convokit==2.5.2) (3.7.0)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=2.3.5->convokit==2.5.2) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit==2.5.2) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit==2.5.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit==2.5.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit==2.5.2) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=2.3.5->convokit==2.5.2) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And download the \"supreme-corpus\" fold from convokit"
      ],
      "metadata": {
        "id": "cht3ErlYqu9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import convokit\n",
        "data_dir = '/content/data/'\n",
        "root_dir = convokit.download('supreme-corpus', data_dir=data_dir)\n",
        "! wget https://zissou.infosci.cornell.edu/convokit/datasets/supreme-corpus/cases.jsonl -O cases.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oee3FZq78bzG",
        "outputId": "866499d9-a01c-49e7-f396-959d21e20ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already exists at /content/data/supreme-corpus\n",
            "--2022-02-16 05:23:06--  https://zissou.infosci.cornell.edu/convokit/datasets/supreme-corpus/cases.jsonl\n",
            "Resolving zissou.infosci.cornell.edu (zissou.infosci.cornell.edu)... 128.253.51.178\n",
            "Connecting to zissou.infosci.cornell.edu (zissou.infosci.cornell.edu)|128.253.51.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13337468 (13M) [application/octet-stream]\n",
            "Saving to: ‘cases.jsonl’\n",
            "\n",
            "cases.jsonl         100%[===================>]  12.72M  27.3MB/s    in 0.5s    \n",
            "\n",
            "2022-02-16 05:23:06 (27.3 MB/s) - ‘cases.jsonl’ saved [13337468/13337468]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load the downloaded corpus and extract utterances from all justices"
      ],
      "metadata": {
        "id": "Kby2_VH_q86W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from convokit import Corpus\n",
        "corpus = Corpus( root_dir )\n"
      ],
      "metadata": {
        "id": "ZVcLfkZk9iiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm \n",
        "import os\n",
        "\n",
        "def get_justices(input_file='/content/cases.jsonl'):\n",
        "    '''Get names of all justices in the dataset'''\n",
        "    all_justices = set()\n",
        "    with open(input_file, \"r\") as inf:\n",
        "        for j in inf:\n",
        "            j = json.loads(j)\n",
        "            if j[\"votes\"] is not None:\n",
        "                for justice in j[\"votes\"].keys():\n",
        "                    all_justices.add(justice)\n",
        "    return all_justices\n",
        "\n",
        "all_justices = get_justices()\n",
        "\n",
        "\n",
        "\n",
        "utterances = [] # build a list of the utterances we are interested in\n",
        "\n",
        "for u in tqdm(corpus.get_utterance_ids()):\n",
        "    u = corpus.get_utterance(u)\n",
        "    if u.speaker.id in all_justices and u.meta[\"case_id\"][0:3] == \"201\":\n",
        "        utterances.append(u)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szHObL8c90yn",
        "outputId": "9033a353-eed5-4f81-f5c2-ae5802a0e2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1700789/1700789 [00:03<00:00, 540282.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the utterances from two Justices: Justice Roberts and Justice Ginsburg"
      ],
      "metadata": {
        "id": "rmG6tmHfrC-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jrj_text_list = []\n",
        "for u in tqdm( utterances ):\n",
        "    if u.speaker.meta['name'] == 'John G. Roberts, Jr.' and 200 < len( u.text ) < 400:\n",
        "        jrj_text_list.append( u.text )\n",
        "\n",
        "rbg_text_list = []\n",
        "for u in tqdm( utterances ):\n",
        "    if 'ginsburg' in u.speaker.meta['name'].lower() and 200 < len( u.text ) < 400:\n",
        "        rbg_text_list.append( u.text )\n",
        "\n",
        "with open( os.path.join('/content/data/jrj', 'text_list.json'), 'w' ) as f:\n",
        "    print(len(jrj_text_list))\n",
        "    json.dump( jrj_text_list, f, indent=4 ) \n",
        "\n",
        "with open( os.path.join('/content/data/rbg', 'text_list.json'), 'w' ) as f:\n",
        "    print(len(rbg_text_list))\n",
        "    json.dump( rbg_text_list, f, indent=4) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhfdAZjY_cKC",
        "outputId": "6715d8b6-5e88-4378-ad49-1a922d7ab238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 78598/78598 [00:00<00:00, 311348.81it/s]\n",
            "100%|██████████| 78598/78598 [00:00<00:00, 502540.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2016\n",
            "1879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unigram Topic Model (LDA)\n",
        "\n",
        "The code in the unigram topic section uses an LDA (Latent Dirichlet Allocation) model and is modified from this [notebook](https://github.com/kapadias/mediumposts/blob/master/natural_language_processing/topic_modeling/notebooks/Introduction%20to%20Topic%20Modeling.ipynb) (credit to the author Shashank Kapadia)"
      ],
      "metadata": {
        "id": "USEGih6-S8ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the text data, let's train a standard topic model (LDA) on the data!"
      ],
      "metadata": {
        "id": "yF0bpO3CvupS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSaiHSvETCn_",
        "outputId": "023d1d35-3014-4f22-f350-89d4177221cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove stopwords for the topic model training"
      ],
      "metadata": {
        "id": "RB1deyXiypQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        # deacc=True removes punctuations\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word in simple_preprocess(str(doc)) \n",
        "             if word not in stop_words] for doc in texts]\n",
        "\n",
        "import json\n",
        "name = 'jrj'\n",
        "text_fname = f'/content/data/{name}/text_list.json'\n",
        "\n",
        "with open(text_fname, 'r') as f:\n",
        "    text_list = json.load(f)\n",
        "print( len(text_list) )\n",
        "\n",
        "data_words = list(sent_to_words(text_list))\n",
        "\n",
        "# remove stop words\n",
        "data_words = remove_stopwords(data_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHIy0guuTl_J",
        "outputId": "f57c093e-ec75-4053-b677-21778465ebbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the topic model dictionary in gensim:"
      ],
      "metadata": {
        "id": "X9duH8T9yvlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.corpora as corpora\n",
        "\n",
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_words)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_words\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n"
      ],
      "metadata": {
        "id": "mZTsnaM7UGCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the LDA topic model!"
      ],
      "metadata": {
        "id": "9Z_VGiKEyzGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# number of topics\n",
        "num_topics = 20\n",
        "\n",
        "# Build LDA model\n",
        "# LdaMulticore from gensim is typically pretty fast for training \n",
        "# But other libraries like Mallet (and there is a gensim wrapper on that) may give better topics \n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=id2word,\n",
        "                                       num_topics=num_topics)\n",
        "\n",
        "# Print the Keyword in the 10 topics\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpus]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRNCzsTsULIh",
        "outputId": "c13d5c2b-282b-4d2a-f9f6-01a9f3c6dbee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.012*\"well\" + 0.010*\"would\" + 0.010*\"know\" + 0.008*\"going\" + 0.008*\"mean\" '\n",
            "  '+ 0.007*\"government\" + 0.006*\"get\" + 0.006*\"case\" + 0.006*\"different\" + '\n",
            "  '0.006*\"one\"'),\n",
            " (1,\n",
            "  '0.073*\"know\" + 0.048*\"mr\" + 0.047*\"stay\" + 0.047*\"granddaughter\" + '\n",
            "  '0.034*\"united\" + 0.033*\"states\" + 0.024*\"get\" + 0.024*\"things\" + 0.024*\"go\" '\n",
            "  '+ 0.024*\"back\"'),\n",
            " (2,\n",
            "  '0.048*\"foreign\" + 0.037*\"entity\" + 0.029*\"case\" + 0.028*\"got\" + '\n",
            "  '0.028*\"different\" + 0.028*\"effective\" + 0.021*\"know\" + 0.015*\"one\" + '\n",
            "  '0.015*\"would\" + 0.014*\"like\"'),\n",
            " (3,\n",
            "  '0.020*\"say\" + 0.016*\"know\" + 0.015*\"well\" + 0.013*\"mean\" + 0.011*\"think\" + '\n",
            "  '0.009*\"want\" + 0.008*\"case\" + 0.008*\"would\" + 0.008*\"right\" + 0.007*\"says\"'),\n",
            " (4,\n",
            "  '0.012*\"well\" + 0.010*\"would\" + 0.006*\"going\" + 0.006*\"statute\" + '\n",
            "  '0.005*\"state\" + 0.005*\"mean\" + 0.005*\"say\" + 0.005*\"may\" + 0.004*\"know\" + '\n",
            "  '0.004*\"says\"'),\n",
            " (5,\n",
            "  '0.138*\"land\" + 0.047*\"words\" + 0.047*\"somebody\" + 0.047*\"anything\" + '\n",
            "  '0.046*\"else\" + 0.046*\"someone\" + 0.046*\"unusual\" + 0.046*\"traversing\" + '\n",
            "  '0.046*\"recognition\" + 0.046*\"belongs\"'),\n",
            " (6,\n",
            "  '0.018*\"well\" + 0.016*\"say\" + 0.010*\"mean\" + 0.009*\"going\" + 0.008*\"one\" + '\n",
            "  '0.008*\"know\" + 0.007*\"case\" + 0.007*\"think\" + 0.007*\"want\" + 0.006*\"could\"'),\n",
            " (7,\n",
            "  '0.012*\"going\" + 0.009*\"know\" + 0.007*\"say\" + 0.006*\"would\" + 0.006*\"says\" + '\n",
            "  '0.006*\"case\" + 0.005*\"mean\" + 0.005*\"federal\" + 0.005*\"want\" + '\n",
            "  '0.005*\"well\"'),\n",
            " (8,\n",
            "  '0.031*\"saying\" + 0.030*\"look\" + 0.030*\"thought\" + 0.030*\"give\" + '\n",
            "  '0.029*\"fact\" + 0.029*\"find\" + 0.029*\"record\" + 0.029*\"example\" + '\n",
            "  '0.029*\"foreign\" + 0.028*\"forth\"'),\n",
            " (9,\n",
            "  '0.013*\"well\" + 0.010*\"law\" + 0.009*\"saying\" + 0.008*\"state\" + 0.008*\"would\" '\n",
            "  '+ 0.008*\"case\" + 0.007*\"think\" + 0.007*\"one\" + 0.006*\"say\" + 0.006*\"know\"'),\n",
            " (10,\n",
            "  '0.015*\"well\" + 0.010*\"right\" + 0.009*\"case\" + 0.008*\"think\" + 0.008*\"mean\" '\n",
            "  '+ 0.006*\"say\" + 0.006*\"state\" + 0.005*\"get\" + 0.005*\"may\" + 0.005*\"says\"'),\n",
            " (11,\n",
            "  '0.037*\"mr\" + 0.025*\"case\" + 0.025*\"international\" + 0.023*\"one\" + '\n",
            "  '0.023*\"domestic\" + 0.022*\"foreign\" + 0.022*\"entity\" + 0.020*\"states\" + '\n",
            "  '0.019*\"united\" + 0.017*\"position\"'),\n",
            " (12,\n",
            "  '0.069*\"area\" + 0.069*\"really\" + 0.035*\"need\" + 0.034*\"gas\" + '\n",
            "  '0.034*\"located\" + 0.034*\"resources\" + 0.034*\"barrier\" + 0.034*\"natural\" + '\n",
            "  '0.034*\"east\" + 0.034*\"pipeline\"'),\n",
            " (13,\n",
            "  '0.015*\"case\" + 0.015*\"know\" + 0.010*\"get\" + 0.009*\"say\" + 0.009*\"court\" + '\n",
            "  '0.009*\"well\" + 0.009*\"would\" + 0.006*\"going\" + 0.006*\"think\" + '\n",
            "  '0.006*\"mean\"'),\n",
            " (14,\n",
            "  '0.014*\"say\" + 0.014*\"know\" + 0.013*\"would\" + 0.011*\"says\" + 0.010*\"well\" + '\n",
            "  '0.007*\"going\" + 0.006*\"go\" + 0.006*\"question\" + 0.006*\"want\" + '\n",
            "  '0.005*\"case\"'),\n",
            " (15,\n",
            "  '0.017*\"say\" + 0.015*\"going\" + 0.011*\"well\" + 0.010*\"would\" + 0.008*\"know\" + '\n",
            "  '0.007*\"go\" + 0.007*\"mean\" + 0.007*\"state\" + 0.007*\"got\" + 0.007*\"think\"'),\n",
            " (16,\n",
            "  '0.067*\"land\" + 0.067*\"maybe\" + 0.067*\"trail\" + 0.067*\"appalachian\" + '\n",
            "  '0.034*\"seems\" + 0.034*\"call\" + 0.034*\"entirely\" + 0.034*\"private\" + '\n",
            "  '0.034*\"guess\" + 0.034*\"suggest\"'),\n",
            " (17,\n",
            "  '0.108*\"com\" + 0.043*\"booking\" + 0.022*\"second\" + 0.022*\"lot\" + '\n",
            "  '0.022*\"using\" + 0.022*\"use\" + 0.022*\"description\" + 0.022*\"services\" + '\n",
            "  '0.022*\"accurate\" + 0.022*\"succeed\"'),\n",
            " (18,\n",
            "  '0.082*\"section\" + 0.082*\"registration\" + 0.043*\"well\" + 0.043*\"one\" + '\n",
            "  '0.042*\"maybe\" + 0.042*\"reason\" + 0.041*\"congress\" + 0.041*\"respect\" + '\n",
            "  '0.041*\"put\" + 0.041*\"significance\"'),\n",
            " (19,\n",
            "  '0.039*\"case\" + 0.038*\"would\" + 0.035*\"know\" + 0.022*\"understand\" + '\n",
            "  '0.022*\"theory\" + 0.022*\"thank\" + 0.022*\"test\" + 0.022*\"cancellation\" + '\n",
            "  '0.022*\"significance\" + 0.022*\"primary\"')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The descriptions on topics using unigram only may not always be formative, espeically when there are complicatd abstract concepts in the corpus. Hence, we would like to add phrases into the mix to help describe the topics."
      ],
      "metadata": {
        "id": "KP0gg--U0d_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phrase-Based Neural Topic Model (PNTM)"
      ],
      "metadata": {
        "id": "6z8_lAEU_6Sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get phrases through consituency parsing"
      ],
      "metadata": {
        "id": "OzI7iGy3C3_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"                 Sentence\n",
        "                     |\n",
        "       +-------------+------------+\n",
        "       |                          |\n",
        "  Noun Phrase                Verb Phrase\n",
        "       |                          |\n",
        "     John                 +-------+--------+\n",
        "                          |                |\n",
        "                        Verb          Noun Phrase\n",
        "                          |                |\n",
        "                        sees              Bill\n",
        "\"\"\"\n",
        "# the above example is taken from: https://stackoverflow.com/a/10401433"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "gVR631lb6I0J",
        "outputId": "ae8e48e7-461b-40a0-f68b-aa0a3108e3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'                 Sentence\\n                     |\\n       +-------------+------------+\\n       |                          |\\n  Noun Phrase                Verb Phrase\\n       |                          |\\n     John                 +-------+--------+\\n                          |                |\\n                        Verb          Noun Phrase\\n                          |                |\\n                        sees              Bill\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install benepar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L43cdh0zARqL",
        "outputId": "eace735e-2bfd-4a5b-8363-2e97d3b629e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: benepar in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from benepar) (0.1.96)\n",
            "Collecting transformers[tokenizers,torch]>=4.2.2\n",
            "  Using cached transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from benepar) (1.10.0+cu111)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.7/dist-packages (from benepar) (3.7)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.7/dist-packages (from benepar) (3.2.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from benepar) (3.17.3)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dependencies to perform constituency chunking"
      ],
      "metadata": {
        "id": "AoSYhey05pKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import json\n",
        "from typing import Counter\n",
        "import spacy\n",
        "import benepar\n",
        "from benepar.integrations.spacy_plugin import SentenceWrapper\n",
        "import os, time"
      ],
      "metadata": {
        "id": "K90o5Yji__E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the saved text data."
      ],
      "metadata": {
        "id": "23XpDJ0V6maq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "name = 'jrj'\n",
        "text_fname = f'/content/data/{name}/text_list.json'\n",
        "\n",
        "with open(text_fname, 'r') as f:\n",
        "    text_list = json.load(f)\n",
        "print( len(text_list) )\n"
      ],
      "metadata": {
        "id": "7BoRnWuxAMv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the constituency parsing process"
      ],
      "metadata": {
        "id": "BR83w-9C6tpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m spacy download en_core_web_md # download the required data for spacy if needed \n",
        "import nltk\n",
        "from tqdm import tqdm\n",
        "benepar.download('benepar_en3')\n",
        "nlp = spacy.load('en_core_web_md',\n",
        "            exclude=['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer'])\n",
        "print('Loaded spacy model')\n",
        "nlp.add_pipe('sentencizer')\n",
        "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "parse_string_list = []\n",
        "for text in tqdm(text_list):\n",
        "    doc = nlp(text)\n",
        "    sent = list(doc.sents)[0]\n",
        "    parse_string = sent._.parse_string\n",
        "    parse_string_list.append( parse_string )\n"
      ],
      "metadata": {
        "id": "PhWENAErAs3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the constituency chunks"
      ],
      "metadata": {
        "id": "SAcmlp756wDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tree import * \n",
        "def ExtractPhrases( myTree, target_types):\n",
        "    myPhrases = []\n",
        "    if (myTree.label() in target_types):\n",
        "        myPhrases.append( myTree.copy(True) )\n",
        "    for child in myTree:\n",
        "        if (type(child) is Tree):\n",
        "            list_of_phrases = ExtractPhrases(child, target_types)\n",
        "            if (len(list_of_phrases) > 0):\n",
        "                myPhrases.extend(list_of_phrases)\n",
        "    return myPhrases\n",
        "\n",
        "target_types = [ 'VP', 'ADJP', 'ADVP', 'PP', 'NP' ]\n",
        "all_result_phrases = []\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "for parse_string in tqdm( parse_string_list ):\n",
        "    t1 = Tree.fromstring(parse_string)\n",
        "    result_phrases = ExtractPhrases(t1, target_types)\n",
        "    all_result_phrases.extend( result_phrases)"
      ],
      "metadata": {
        "id": "CrDfsSw7BDUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the constituency chunks collection as our phrases set"
      ],
      "metadata": {
        "id": "5gOOeUYN6zm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_counter = Counter()\n",
        "for phrase in all_result_phrases:\n",
        "    if len(phrase.leaves()) > 1: # if the constituency is a unigram, then we skip it\n",
        "        label = phrase.label()\n",
        "        phrase_str = ' '.join( phrase.leaves() )\n",
        "        phrase_str = phrase_str.lower()\n",
        "        result_counter.update([phrase_str])\n",
        "\n",
        "with open( f'/content/data/{name}/phrase_counter.pkl', 'wb') as f:\n",
        "    print(len(result_counter))\n",
        "    pickle.dump(result_counter, f)"
      ],
      "metadata": {
        "id": "Ly-hp7QZCz0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_counter.most_common(10)"
      ],
      "metadata": {
        "id": "4VoFarAaDQKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get unigrams through tokenizing"
      ],
      "metadata": {
        "id": "ta6IJ7MXFBHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import string\n",
        "import pickle, json\n",
        "from tqdm import tqdm\n",
        "from transformers import BasicTokenizer\n",
        "\n",
        "justice_name = 'rbg'\n",
        "save_fname = f'/content/data/{justice_name}/unigram_counter.pkl'\n",
        "with open(f'/content/data/{justice_name}/text_list.json', 'r') as f:\n",
        "    text_list = json.load(f)\n",
        "\n",
        "tokenizer = BasicTokenizer()\n",
        "\n",
        "doc_tokens_list = [] # a nested list\n",
        "for doc in tqdm( text_list ):\n",
        "    doc_tokens = tokenizer.tokenize(doc)\n",
        "    filtered_doc_tokens = []\n",
        "    for t in doc_tokens:\n",
        "        if t in string.punctuation:\n",
        "            continue\n",
        "        if t in string.digits:\n",
        "            continue\n",
        "        if len(t) == 1:\n",
        "            continue\n",
        "        filtered_doc_tokens.append( t )\n",
        "    doc_tokens_list.extend(filtered_doc_tokens)\n",
        "\n",
        "from collections import Counter\n",
        "unigram_counter = Counter(doc_tokens_list)\n",
        "with open(save_fname, 'wb') as f:\n",
        "    pickle.dump(unigram_counter, f)\n",
        "\n",
        "print( unigram_counter.most_common(10) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_vtM-oDFIiE",
        "outputId": "209fcb7d-ed9f-4309-aeec-025e633d327a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1879/1879 [00:03<00:00, 543.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 6859), ('that', 3509), ('to', 2548), ('you', 2329), ('it', 2205), ('is', 2044), ('of', 1722), ('and', 1685), ('in', 1477), ('was', 1028)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine the phrases and the unigrams to form the vocab set"
      ],
      "metadata": {
        "id": "fMdJXypjFh8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "justice_name = 'jrj'\n",
        "outdir = f'/content/data/{justice_name}/'\n",
        "unigram_fname = f'/content/data/{justice_name}/unigram_counter.pkl'\n",
        "phrase_fname = f'/content/data/{justice_name}/phrase_counter.pkl'\n",
        "with open(unigram_fname, 'rb') as f:\n",
        "    unigrams_counter = pickle.load(f)\n",
        "\n",
        "with open(phrase_fname, 'rb') as f:\n",
        "    phrase_counter = pickle.load(f)\n",
        "\n",
        "print( f'The number of unigrams loaded: {len(unigrams_counter)}' )\n",
        "print( f'The Number of phrases loaded: {len(phrase_counter)}' )\n",
        "\n",
        "vocab_list = [ k for k, v in unigrams_counter.items()] + \\\n",
        "    [ k for k, v in phrase_counter.items()]\n",
        "vocab_list = list(set(vocab_list))\n",
        "print(len(vocab_list))\n",
        "\n",
        "word2id_dict = {}\n",
        "id2word_dict = {}\n",
        "id2freq_dict = {}\n",
        "for id, vocab in enumerate(vocab_list):\n",
        "    word2id_dict[vocab] = id\n",
        "    id2word_dict[id] = vocab\n",
        "\n",
        "for id, vocab in id2word_dict.items():\n",
        "    id2freq_dict[id] = unigrams_counter[vocab] if vocab in unigrams_counter else phrase_counter[vocab]\n",
        "\n",
        "print( f'The number of vocabualries (phrases + unigrams pooled): {len(word2id_dict)}')\n",
        "print( f'The number of vocabualries (phrases + unigrams pooled): {len(id2word_dict)}')\n",
        "print( f'The number of vocabualries (phrases + unigrams pooled): {len(id2freq_dict)}')\n",
        "\n",
        "import os\n",
        "with open( os.path.join(outdir, 'combined_word2id_dict.pkl'), 'wb') as f:\n",
        "    pickle.dump(word2id_dict, f)\n",
        "\n",
        "with open( os.path.join(outdir, 'combined_id2word_dict.pkl'), 'wb') as f:\n",
        "    pickle.dump(id2word_dict, f)\n",
        "\n",
        "with open( os.path.join(outdir, 'id2freq_dict.pkl'), 'wb') as f:\n",
        "    pickle.dump(id2freq_dict, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v2NdK39FUtH",
        "outputId": "d790f18b-a80a-4cf5-e1e0-b8abc4b2a1c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of unigrams loaded: 6111\n",
            "The Number of phrases loaded: 20457\n",
            "26568\n",
            "The number of vocabualries (phrases + unigrams pooled): 26568\n",
            "The number of vocabualries (phrases + unigrams pooled): 26568\n",
            "The number of vocabualries (phrases + unigrams pooled): 26568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Phrase-BERT to produce embeddings for input text and vocabularies"
      ],
      "metadata": {
        "id": "FXexU_6-GhQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==3.0.2 # the constituency parsing process earlier requires transformers version 4.6.1 so we have to reinstall the correct one\n",
        "# Having multiple reinstallations is not great. If you have conflicting dependencies in your project, you may use virtual environments and anaconda."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97Lq3XwRH_gn",
        "outputId": "49cb472f-8ac3-47d5-9671-0c22c1f873a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.96)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.0.47)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2022.1.18)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.62.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.21.5)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.8.1rc1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -u /content/phrase-bert-topic-model/phrase-topic-model/preprocess.py \\\n",
        "    --topic_model_data_path \"/content/data/rbg/\" \\\n",
        "    --emb_model_path \"/content/phrase-bert-model/pooled_context_para_triples_p=0.8\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBV6xpioG2wS",
        "outputId": "e9b9f2f8-798d-4dd7-9f09-3c7bd3780890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded 25434 vocabs\n",
            "25434\n",
            "Batches: 100% 3180/3180 [01:12<00:00, 44.13it/s]\n",
            "1879\n",
            "Batches: 100% 235/235 [00:19<00:00, 12.26it/s]\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python -u /content/phrase-bert-topic-model/phrase-topic-model/run_topic_model.py \\\n",
        "    --num_topics 20 \\\n",
        "    --num_epochs 100 \\\n",
        "    --random_seed 42 \\\n",
        "    --topic_model_data_path \"/content/data/rbg/\" \\\n",
        "    --emb_model phrase-bert > \"/content/data/rbg/output.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TV8LtVJHKxV",
        "outputId": "ccdb9696-bb70-4804-915e-736b6f8da79d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0/5 [00:00<?, ?it/s]\r100% 5/5 [00:00<00:00, 422.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topics produced by PNTM:"
      ],
      "metadata": {
        "id": "zAJQAi1RB-g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# topic 0 : the legislature, in the north carolina statute, congress, the puerto rico government, imposed under the internal revenue codes, executive , legislature, under the internal revenue codes, government as sovereign, the legislature and the government, congress , the legislature ,\n",
        "# topic 1 : to live with one 's spouse, between males and females, disabilities, sexual abuse, women of child - bearing age, sexual abuse of an adult, gender discrimination, with disabilities, gender, men and women who are parents\n",
        "# topic 2 : set standards for emissions, 25 minutes, using the taser, contains hazardous substances, hazardous substances, over the standard amount, a lethal injection protocol, a federal safety standard, this questionable drug, narcotics\n",
        "# topic 3 : the very basic argument, technicality, argument 's sake, specification, formulation, evidentiary, argument 's, the essential argument, the simple argument, the technicality\n",
        "# topic 4 : the internal revenue codes, banks, the consumer finance protection bureau, distributor, bankshares, shopkeeping, export administration, bank, the bank, the government regulators\n",
        "# topic 5 : paid the lawyer, noticed, his other money, gave the proceeds to his, had all of his other money, paid out of trust funds, was very nervous, asked for that relief, on my income tax return, my income tax return\n",
        "# topic 6 : the federal rules, any specific jurisdiction case, an ordinary litigation in federal court, the federal law, choice of law, federal law, by a federal common law rule, dealing with choice of law, the court 's rationale, a federal common law rule\n",
        "# topic 7 : make an arrest, be stopped, make the stop, intercept, the stop, pass, get out at the same time, commence, sign, to pass\n",
        "# topic 8 : a domestic assault, employer, inherent authority, this obligation, rests on an employer acting unlawfully, the offense of possession, an unlawful employment practice, the inherent authority, on an employer acting unlawfully, an employer acting unlawfully\n",
        "# topic 9 : to 30 percent, on the 10 percent, 30 within 30, the 10 - year limitation, ten years mandatory minimum for that, this 3 - year outside limit, require further reductions, at the foreclosure amount, trebled the amount at the outset, was additional insurance in 1028\n",
        "# topic 10 : suspected, the pretrial detainee, the unlawful detention, contains hazardous substances, traceable, mentioned the pretrial detainee, violation of a federal safety standard, this questionable drug, unlawful entry, detainee\n",
        "# topic 11 : embassy, foreign governments, need not be a government officer, foreign plaintiffs, deference to state domestic relations law, the iran u.s. claims commission, the foreign state, governed by foreign law, a resident alien, state domestic relations law\n",
        "# topic 12 : escape, the inmate contraband, to suffer pain, work out the back pay, getting disability pay, induce this unconscious state, detainee, pay or compensation, be possible to find the victims, be likely subject to torture\n",
        "# topic 13 : commencement of a lawsuit, a state claim for negligence, a conviction entered by a court, the statutory damages, breach of fiduciary claims, the compensatory damages, be brought as a class action, for breach of fiduciary claims, an implied damages action, punitive damages\n",
        "# topic 14 : gone into court, be a debt owed to, an effort to get a warrant, get a warrant, the prosecutor, have the informant 's tip, for false arrest, a debt owed to, a warrant, the informant 's tip\n",
        "# topic 15 : parole, can release the prisoner, the minimum term of imprisonment, release the prisoner, arrest, go to trial, have a criminal trial, plead guilty, into an arrest, make an arrest\n",
        "# topic 16 : on court, the lawyer for the defense, injuring california people, tough, a lawyer, any careful lawyer, bad things, the lawyer, paid the lawyer, do bad things\n",
        "# topic 17 : use the word \" person, husband, father / child relationships, men and women who are parents, want, spouse, live with one 's spouse, your spouse, to live with one 's spouse, for the wife\n",
        "# topic 18 : 's what this court has declared, entered by a court, decree, by its sovereign immunity, a petition filed, its sovereign immunity, was announced by this court, before foreign tribunals, the mandate issued, announced by this court\n",
        "# topic 19 : was he wanted the testimony, asked before, have the informant 's tip, about the police officer, ask what i assume was, was not telling them, want to go to trial, go to trial, suppose he had survived, for suspicion\n"
      ],
      "metadata": {
        "id": "q29AMkKELvQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing PNTM vs LDA:"
      ],
      "metadata": {
        "id": "HytCT8PpCBck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the \"international\" topic from LDA:\n",
        "# mr, case, international, one, domestic, foreign, entity, states, united, position\n",
        "\n",
        "# the \"international\" topic from PNTM: \n",
        "# embassy, foreign governments, need not be a government officer, foreign plaintiffs, deference to state domestic relations law, \n",
        "# the iran u.s. claims commission, the foreign state, governed by foreign law, a resident alien, state domestic relations law\n",
        "\n"
      ],
      "metadata": {
        "id": "-_UwSaue7jvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing two PNTM-produced topics:"
      ],
      "metadata": {
        "id": "Js4FbP1OCDn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# the \"finance\" topic from PNTM (Justice Roberts):\n",
        "# monetary, the compensation, pensions, payment, paycheck, money remuneration, financial, budgeting, budget, the hourly wage\n",
        "\n",
        "# the \"finance\" topic from PNTM (Justice Ginsburg):\n",
        "# the internal revenue codes, banks, the consumer finance protection bureau, distributor, bankshares, shopkeeping, export administration, bank, the bank, the government regulators\n"
      ],
      "metadata": {
        "id": "4Dn4WzOy8XKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HhXBmZxIupvN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}